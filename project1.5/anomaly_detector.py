import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import sounddevice as sd
import librosa
import os
import logging
import joblib
from dataclasses import dataclass
from sklearn.linear_model import SGDOneClassSVM
from sklearn.preprocessing import StandardScaler
from scipy import signal
from threading import Thread, Lock
from datetime import datetime
from tenacity import retry, stop_after_attempt, wait_exponential
from typing import Optional, List
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import use as mpl_use
import warnings
from librosa.util.exceptions import ParameterError
from matplotlib.animation import FuncAnimation


os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

# Matplotlib ì„¤ì •
mpl_use('Agg')
matplotlib.use('TkAgg')
matplotlib.rcParams['toolbar'] = 'None'

# ì„¤ì • í´ë˜ìŠ¤
@dataclass
class AudioConfig:
    sample_rate: int
    duration: int
    channels: int
    yamnet_sample_rate: int
    buffer_size: int

@dataclass
class ModelConfig:
    contamination: str
    n_estimators: int
    threshold_percentile: int
    grid_search: bool
    update_interval: float
    use_autoencoder: bool
    autoencoder_epochs: int
    feature_dim: int
    bandpass_filter: bool
    augmentation: bool

@dataclass
class Config:
    audio: AudioConfig
    model: ModelConfig
    dataset_path: str
    model_path: str

# ì˜¤ë””ì˜¤ ë²„í¼ í´ë˜ìŠ¤
class RingBuffer:
    def __init__(self, max_size: int, frame_size: int):
        self.max_size = max_size
        self.frame_size = frame_size
        self.buffer = np.zeros((max_size, frame_size), dtype=np.float32)
        self.index = 0
        self.lock = Lock()

    def put(self, data: np.ndarray):
        flattened = data.squeeze()
        with self.lock:
            if len(flattened) == self.frame_size:
                self.buffer[self.index % self.max_size] = flattened
                self.index += 1

    def get(self) -> Optional[np.ndarray]:
        with self.lock:
            if self.index == 0:
                return None
            idx = (self.index - 1) % self.max_size
            return self.buffer[idx]

# ì½”ì–´ ë¡œì§ í´ë˜ìŠ¤
class AccidentSoundAnomalyDetector:
    def __init__(self, config: Config):
        self.config = config
        self._init_gpu()
        self._load_yamnet()
        self._init_models()
        self.audio_buffer = RingBuffer(
            max_size=config.audio.buffer_size,
            frame_size=int(config.audio.sample_rate * config.audio.duration)
        )
        self.scaler = StandardScaler()
        self.logger = self._configure_logger()

    def _process_audio(self, audio: np.ndarray, sr: int) -> List[np.ndarray]:
        target_len = int(sr * self.config.audio.duration)
        chunks = []
        # ë„ˆë¬´ ì§§ì€ ê²½ìš°: íŒ¨ë”© ë˜ëŠ” ë°˜ë³µ
        if len(audio) < target_len:
            if len(audio) > 0:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì²˜ë¦¬
                padding_needed = target_len - len(audio)
                if padding_needed <= len(audio):  # ì§§ì€ ê²½ìš° íŒ¨ë”©
                    audio = np.pad(audio, (0, padding_needed), mode='constant')
                else:  # ë§¤ìš° ì§§ì€ ê²½ìš° ë°˜ë³µ
                    repeat_count = int(np.ceil(target_len / len(audio)))
                    audio = np.tile(audio, repeat_count)[:target_len]
            chunks.append(audio)
        # ê¸´ ê²½ìš°: ì˜ë¼ì„œ ì²­í¬ë¡œ ë¶„í• 
        else:
            num_chunks = int(np.floor(len(audio) / target_len))
            for i in range(num_chunks):
                start = i * target_len
                end = start + target_len
                chunks.append(audio[start:end])
            # ë‚¨ì€ ë¶€ë¶„ ì²˜ë¦¬ (ì§§ì€ ìƒ˜í”Œ í¬í•¨)
            remaining = audio[num_chunks * target_len:]
            if len(remaining) > 0:
                if len(remaining) < target_len:
                    remaining = np.pad(remaining, (0, target_len - len(remaining)), mode='constant')
                chunks.append(remaining)
        return chunks

    def _init_gpu(self):
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            try:
                tf.config.experimental.set_memory_growth(gpus[0], True)
                self.logger.info("GPU ë©”ëª¨ë¦¬ ë™ì  í• ë‹¹ í™œì„±í™”")
            except RuntimeError as e:
                self.logger.warning(f"GPU ì„¤ì • ì‹¤íŒ¨: {e}")

    def _load_yamnet(self):
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            self.yamnet = hub.load('https://tfhub.dev/google/yamnet/1')

    def _init_models(self):
        self.ocsvm = SGDOneClassSVM(
            nu=0.1,
            shuffle=True,
            tol=1e-4,
            max_iter=1000
        )
        self.autoencoder = self._build_autoencoder()
        self.threshold = None

    def _build_autoencoder(self):
        input_dim = self.config.model.feature_dim
        encoder_input = tf.keras.layers.Input(shape=(input_dim,))
        # Encoder
        x = tf.keras.layers.Dense(64, activation='relu')(encoder_input)
        x = tf.keras.layers.Dense(32, activation='relu')(x)
        # Decoder
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        decoder_output = tf.keras.layers.Dense(input_dim, activation='sigmoid')(x)
        return tf.keras.Model(inputs=encoder_input, outputs=decoder_output)

    def _configure_logger(self):
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
            logger.addHandler(handler)
        return logger

    def _normalize_audio(self, audio: np.ndarray) -> np.ndarray:
        rms = np.sqrt(np.mean(audio**2))
        audio = audio / (rms + 1e-6)
        return np.sign(audio) * np.log1p(np.abs(audio))

    def _apply_bandpass_filter(self, audio: np.ndarray, sr: int) -> np.ndarray:
        if self.config.model.bandpass_filter:
            nyq = 0.5 * sr
            low = 300 / nyq
            high = 4000 / nyq
            b, a = signal.butter(4, [low, high], btype='band')
            return signal.lfilter(b, a, audio)
        return audio

    def _augment_audio(self, audio: np.ndarray, sr: int) -> np.ndarray:
        if self.config.model.augmentation:
            # Pitch Shift
            if np.random.rand() > 0.5:
                audio = librosa.effects.pitch_shift(audio, sr=sr, n_steps=np.random.uniform(-2, 2))
            # Time Stretch
            if np.random.rand() > 0.5:
                rate = np.random.uniform(0.8, 1.2)
                audio = librosa.effects.time_stretch(audio, rate=rate)
            # ë…¸ì´ì¦ˆ ì¶”ê°€
            if np.random.rand() > 0.5:
                audio = audio + np.random.normal(0, 0.005, audio.shape)
            # ì‹œê°„ ì´ë™
            if np.random.rand() > 0.5:
                shift = np.random.randint(-sr // 2, sr // 2)
                audio = np.roll(audio, shift)
            # ëª©í‘œ ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°
            target_len = self.config.audio.yamnet_sample_rate * self.config.audio.duration
            if len(audio) < target_len:
                audio = np.pad(audio, (0, target_len - len(audio)))
            else:
                audio = audio[:target_len]
        return audio

    def _extract_features(self, audio: np.ndarray, sr: int) -> np.ndarray:
        # Mel-spectrogram ì¶”ì¶œ
        S = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)
        log_S = librosa.power_to_db(S, ref=np.max)
        # MFCC ë° ê¸°íƒ€ íŠ¹ì§• ì¶”ì¶œ
        mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=20)
        mfcc_delta = librosa.feature.delta(mfcc)
        spectral_centroid = librosa.feature.spectral_centroid(S=S)
        # ê° íŠ¹ì§•ì˜ í‰ê· ê°’ ê³„ì‚°
        features = np.concatenate([
            np.mean(mfcc, axis=1),
            np.mean(mfcc_delta, axis=1),
            np.mean(spectral_centroid, axis=1),
            np.mean(log_S, axis=1)
        ])
        # feature_dimì— ë§ê²Œ í¬ê¸° ì¡°ì •
        target_dim = self.config.model.feature_dim
        if len(features) > target_dim:
            features = features[:target_dim]  # ì´ˆê³¼í•˜ëŠ” ê²½ìš° ì˜ë¼ëƒ„
        elif len(features) < target_dim:
            features = np.pad(features, (0, target_dim - len(features)))  # ë¶€ì¡±í•œ ê²½ìš° íŒ¨ë”©
        return features

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1))
    def preprocess_audio(self, audio: np.ndarray, sr: int) -> np.ndarray:
        try:
            audio = self._normalize_audio(audio)
            audio = self._apply_bandpass_filter(audio, sr)
            if sr != self.config.audio.yamnet_sample_rate:
                audio = librosa.resample(
                    audio,
                    orig_sr=sr,
                    target_sr=self.config.audio.yamnet_sample_rate,
                    res_type='kaiser_best'
                )
            target_len = self.config.audio.yamnet_sample_rate * self.config.audio.duration
            if len(audio) < target_len:
                audio = np.pad(audio, (0, target_len - len(audio)))
            else:
                audio = audio[:target_len]
            return audio
        except Exception as e:
            self.logger.error(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise

    def train(self, new_samples_only=False):
        X = []
        file_list = [f for f in os.listdir(self.config.dataset_path) 
                     if f.lower().endswith(('.wav', '.mp3'))]
        if not file_list:
            raise ValueError("í•™ìŠµ ë°ì´í„° ì—†ìŒ")
        if new_samples_only and os.path.exists(self.config.model_path):
            self.load_model()
            self.logger.info("ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ì¶”ê°€ í•™ìŠµ ì‹œì‘...")
        else:
            self.logger.info("ìƒˆë¡œìš´ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        for fname in file_list:
            path = os.path.join(self.config.dataset_path, fname)
            try:
                audio, sr = librosa.load(path, sr=self.config.audio.yamnet_sample_rate)
                processed_chunks = self._process_audio(audio, sr)
                for chunk in processed_chunks:
                    chunk = self._augment_audio(chunk, sr)
                    features = self._extract_features(chunk, sr)
                    X.append(features)
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {fname} - {str(e)}")
        if len(X) == 0:
            raise ValueError("í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        # ë°ì´í„° í‘œì¤€í™”
        X = self.scaler.fit_transform(X)
        # Autoencoder í•™ìŠµ
        if self.config.model.use_autoencoder:
            self.autoencoder.compile(optimizer='adam', loss='mse')
            self.autoencoder.fit(
                X, X,
                epochs=self.config.model.autoencoder_epochs,
                batch_size=32,
                verbose=0
            )
        # OCSVM í•™ìŠµ
        self.ocsvm.fit(X)
        self._set_threshold(X)
        self._save_model()

    def _set_threshold(self, X: np.ndarray):
        scores = self.ocsvm.score_samples(X)
        self.threshold = np.percentile(
            scores, 
            100 - self.config.model.threshold_percentile
        )
        self.logger.info(f"ì„ê³„ê°’ ì„¤ì • ì™„ë£Œ: {self.threshold:.2f}")

    def _save_model(self):
        os.makedirs(os.path.dirname(self.config.model_path), exist_ok=True)
        joblib.dump({
            'model': self.ocsvm,
            'autoencoder': self.autoencoder.get_weights() if self.config.model.use_autoencoder else None,
            'threshold': self.threshold,
            'scaler': self.scaler
        }, self.config.model_path)
        self.logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.config.model_path}")

    def load_model(self):
        data = joblib.load(self.config.model_path)
        self.ocsvm = data['model']
        self.threshold = data['threshold']
        self.scaler = data['scaler']
        if self.config.model.use_autoencoder and data['autoencoder'] is not None:
            self.autoencoder.set_weights(data['autoencoder'])
        self.logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def predict(self, audio: np.ndarray, sr: int) -> dict:
        try:
            processed_audio = self.preprocess_audio(audio, sr)
            features = self._extract_features(processed_audio, sr)
            features_scaled = self.scaler.transform([features])
            # OCSVM ì ìˆ˜
            ocsvm_score = self.ocsvm.score_samples(features_scaled)[0]
            # Autoencoder ì ìˆ˜
            ae_score = 0.0
            if self.config.model.use_autoencoder:
                reconstructed = self.autoencoder.predict(features_scaled, verbose=0)
                ae_score = np.mean(np.square(features_scaled - reconstructed))
            # ì•™ìƒë¸” ì ìˆ˜ ê³„ì‚°
            combined_score = ocsvm_score - (ae_score * 0.3 if self.config.model.use_autoencoder else 0)
            is_accident = combined_score > self.threshold
            return {
                'is_accident': is_accident,
                'score': combined_score,
                'waveform': processed_audio,
                'mel_spectrogram': librosa.power_to_db(
                    librosa.feature.melspectrogram(y=processed_audio, sr=sr),
                    ref=np.max
                )
            }
        except Exception as e:
            self.logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return {'is_accident': False, 'score': 0}

# ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
class AudioMonitor:
    def __init__(self, detector: AccidentSoundAnomalyDetector, config: Config):
        self.detector = detector
        self.config = config.audio
        self.model_config = config.model
        self.is_running = False
        self.stream = None
        self.last_status = None
        self.last_update_time = 0
        self.active_figures = []
        self.max_figures = 5

        # ì‹¤ì‹œê°„ ê·¸ë˜í”„ ì´ˆê¸°í™”
        self.fig, (self.ax_wave, self.ax_spec) = plt.subplots(2, 1, figsize=(12, 6))
        self.line_wave, = self.ax_wave.plot([], [])
        self.spec_img = self.ax_spec.imshow(np.zeros((128, 128)), aspect='auto', origin='lower', cmap='inferno')
        self.ax_wave.set_title("Waveform")
        self.ax_wave.set_xlabel("Time (s)")
        self.ax_wave.set_ylabel("Amplitude")
        self.ax_spec.set_title("Mel Spectrogram")
        self.ax_spec.set_xlabel("Time")
        self.ax_spec.set_ylabel("Frequency")
        plt.colorbar(self.spec_img, ax=self.ax_spec, format='%+2.0f dB')
        plt.tight_layout()

    def _callback(self, indata, frames, time, status):
        if status:
            logging.warning(f"ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {status}")
        self.detector.audio_buffer.put(indata.copy().squeeze())

    def start(self):
        import time
        self.is_running = True
        self.stream = sd.InputStream(
            samplerate=self.config.sample_rate,
            channels=self.config.channels,
            callback=self._callback,
            blocksize=int(self.config.sample_rate * self.config.duration)
        )
        self.stream.start()
        logging.info("ëª¨ë‹ˆí„°ë§ ì‹œì‘... (Ctrl+C ì¢…ë£Œ)")

        # Matplotlib ì• ë‹ˆë©”ì´ì…˜ ì„¤ì •
        ani = FuncAnimation(self.fig, self._update_plot, interval=500, blit=False)
        plt.show()

    def _update_plot(self, frame):
        audio = self.detector.audio_buffer.get()
        if audio is None:
            # ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ê·¸ë˜í”„ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€
            return self.line_wave, self.spec_img

        result = self.detector.predict(audio, self.config.sample_rate)
        waveform = result['waveform']
        mel_spec = result['mel_spectrogram']
        is_accident = result['is_accident']

        # Waveform ì—…ë°ì´íŠ¸
        time_axis = np.linspace(0, len(waveform) / self.config.sample_rate, len(waveform))
        self.line_wave.set_data(time_axis, waveform)
        self.ax_wave.relim()  # ì¶• ë²”ìœ„ ì¬ì„¤ì •
        self.ax_wave.autoscale_view()

        # Mel Spectrogram ì—…ë°ì´íŠ¸
        self.spec_img.set_data(mel_spec)
        self.spec_img.set_clim(vmin=np.min(mel_spec), vmax=np.max(mel_spec))

        # ì½˜ì†”ì— ìƒíƒœ ì¶œë ¥
        status_text = 'ğŸš¨ ì‚¬ê³ ' if is_accident else 'âœ… ì •ìƒ'
        timestamp = datetime.now().strftime('%H:%M:%S')
        log = f"[{timestamp}] {status_text} | ì‹ ë¢°ë„: {result['score']:.2f}"
        print(log)

        
        return self.line_wave, self.spec_img

    def stop(self):
        self.is_running = False
        if self.stream:
            self.stream.stop()
            self.stream.close()
        print("\n")
        logging.info("ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
        plt.close('all')